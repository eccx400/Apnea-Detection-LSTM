{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vccheng2001/Apnea-Detection-LSTM/blob/master/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62fLXxvCtPSc"
      },
      "source": [
        "# Building Your Own Optimizers\n",
        "Optimizers tie the loss function and model parameters together by updating the model in response to the output of the loss function. In simpler terms, optimizers shape your model into its most accurate possible form by futzing with the weights. In even more simpler terms, optimizers define the way you want your model to be trained. \n",
        "\n",
        "The selection of optimizers could have dramatic effect on the performance of your model. If you choose a complex optimizer for a simple model, you would be likely to spend much more time in training with barely improved accuracy. And if you choose a naive optimizer for a complicated model, you would probably end up with poor accuracy.  \n",
        "\n",
        "This exercise will cover:\n",
        "- *Part 1: Calling existing optimizers provided by tf.keras*\n",
        "- *Part 2: Implementing and evaluating your own optimizers*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFFfr0xJymEU"
      },
      "source": [
        "## Part 1: Calling existing optimizers provided by tf.keras\n",
        "In this part, we will build up a demo pipeline, demonstrating how to involve optimizers given by `tf.keras` into the training process. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK-Y1FCJzMAD"
      },
      "source": [
        "## (1a) Setting up the environment\n",
        "Just run the following cell to establish the runtime environment. Note that we're using Tensorflow 2.0 for this part of assignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EyUqv7tznpQ"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# If running locally, comment out the following try-except block. \n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.ops import math_ops\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq1P1vvkz7J-"
      },
      "source": [
        "## (1b) Building a baseline model\n",
        "In this task, you will create a simple Linear Regression model with a synthetic toy dataset. Mean Squared Error (MSE) will be used as the loss metric. \n",
        "\n",
        "Note we have set a seed for `tf.random` for auto grading purpose. Please don't change the seed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "YGJTEMT01sP0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e73462dde769e73dba5e6793e8220633",
          "grade": false,
          "grade_id": "LR_MSE",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# A toy dataset of points around y = 4 * x_1 + 3 * x_2 + 2\n",
        "NUM_EXAMPLES = 2000\n",
        "tf.random.set_seed(10605)  \n",
        "X_train = tf.random.normal([NUM_EXAMPLES, 2])\n",
        "noise = tf.random.normal([NUM_EXAMPLES, 1])\n",
        "y_train = tf.matmul(X_train, [[4.], [3.]]) + 2 + noise\n",
        "\n",
        "# Definition of the Linear Regression model\n",
        "class Linear(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Linear, self).__init__()\n",
        "    self.w = tf.Variable([[5., 5.]], name='weight')\n",
        "    self.b = tf.Variable(10., name='bias')\n",
        "\n",
        "  # Implementation of Linear Regression\n",
        "  def call(self, inputs):\n",
        "    \"\"\"\n",
        "      Implementation of Linear Regression. This function will use current parameters to predict the labels for inputs\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          inputs (tf.Tensor): The input tensor of this Linear Regression model.\n",
        "\n",
        "      Returns:\n",
        "          tf.Tensor: Predicted label\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred = tf.add(tf.matmul(inputs, self.w, transpose_b=True), self.b)\n",
        "    return y_pred\n",
        "\n",
        "# MSE loss function\n",
        "def loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "    Calculate MSE loss between the predicted label and the actual label.\n",
        "\n",
        "    Note:\n",
        "        You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "    Args:\n",
        "        y_true (tf.Tensor): The actual label.\n",
        "        y_pred (tf.Tensor): The predicted label.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: MSE loss value\n",
        "  \"\"\"\n",
        "  mse_loss = tf.reduce_mean(tf.square(y_true - y_pred), keepdims=True)\n",
        "  return mse_loss"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QGIWie3vB05w",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f08b00caed24004bec31250b676ff240",
          "grade": true,
          "grade_id": "test_LR_MSE",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test for LR_MSE\n",
        "\n",
        "\n",
        "m, x, y = Linear(), tf.constant([[1.0, 2.0]]), tf.constant([3.0])\n",
        "assert np.equal(m(x).numpy()[0][0], 25.0), \"Wrong implementation of Linear Regression\"\n",
        "assert np.equal(loss(y,m(x)).numpy(), 484.0), \"Wrong implementation of MSE loss\"\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e7iwBiMKNW3"
      },
      "source": [
        "## (1c) Applying an existing optimizer\n",
        "Next, we will apply an Adam optimizer provided by `tf.keras` to our previously defined Linear Regression model. You can reuse this code snippet to validate your own optimizers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDTTt3FtNEJY",
        "outputId": "2d5f4ed3-2c9b-4fd9-c4df-cbdc6427057c"
      },
      "source": [
        "model = Linear()\n",
        "# Standard method to compile a tf.keras.Model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss=loss)\n",
        "# Standard method to fit a tf.keras.Model\n",
        "print(f\"X_train shape: {tf.shape(X_train)}\")\n",
        "print(f\"y_train shape: {tf.shape(y_train)}\")\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10)\n",
        "print(f\"model.w shape: {model.w.shape}\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: [2000    2]\n",
            "y_train shape: [2000    1]\n",
            "Epoch 1/10\n",
            "63/63 [==============================] - 0s 945us/step - loss: 46.4401\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 4.6619\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.0225\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 0s 932us/step - loss: 1.0149\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 0s 936us/step - loss: 0.9971\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 0s 759us/step - loss: 0.9725\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.9351\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 0s 841us/step - loss: 0.9766\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 1.0021\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.9755\n",
            "model.w shape: (1, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InSZvNu4RFFf"
      },
      "source": [
        "## Part 2: Implementing your own optimizers\n",
        "In this part, you will implement four different optimizers by yourself! Don't be panic if you're not good at math stuff, we will give you enough instructions and explanations!\n",
        "\n",
        "Generally, an optimizer class should contain three methods:\n",
        "- `__init__`: Initializes required parameters. \n",
        "-  `calculate_gradient`: Calculates gradients in the computational graph. \n",
        "- `apply_gradient`: Updates parameters of the model. \n",
        "\n",
        "Some of you may notice that we're not using the \"keras-style\" to implement our optimizers. Since keras is a high-level encapsulation, its implementation of optimizers is generally too abstract for you to get the mathematical essence. This hacky way can enable you to design your own optimizers in a lower, Tensorflow level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yvaOmmUTyjz"
      },
      "source": [
        "## (2a) Gradient Descent\n",
        "Let's start with the simplest Gradient Descent. \n",
        "\n",
        "For a Linear Regression model with MSE where $\\mathbf{y}=w^T\\mathbf{X} + b$, the update rules of parameters at the $t$ th epoch are as follows:\n",
        "\n",
        "$$w^{(t+1)}=w^{(t)}-\\alpha \\sum_{i=1}^{n} 2 \\cdot (h_{w,b}(\\mathbf{X}^{(i)}) - \\mathbf{y}^{(i)}) \\mathbf{X}^{(i)}$$\n",
        "\n",
        "$$b^{(t+1)}=b^{(t)}-\\alpha \\sum_{i=1}^{n} 2 \\cdot (h_{w,b}(\\mathbf{X}^{(i)}) - \\mathbf{y}^{(i)}))$$\n",
        "\n",
        "where $n$ is the size of input dataset. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "Iv5EvSVQagfz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9754bdc6190cf522677ec2e76afc24cb",
          "grade": false,
          "grade_id": "GD",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "class GD():\n",
        "  def __init__(self, lr=0.01):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "    \"\"\"\n",
        "    self.lr = lr \n",
        "\n",
        "  def calculate_gradient(self, X, y, model):\n",
        "      \n",
        "    # number of samples\n",
        "    N = len(X)\n",
        "    # difference between predicted, actual\n",
        "    diff = model(X) - y\n",
        "   \n",
        "    # gradients\n",
        "    grads_w = 2/N *  (diff*X)\n",
        "    grads_b =  2/N * diff\n",
        "\n",
        "    return grads_w, grads_b\n",
        "\n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        " \n",
        "    # update weight, bias\n",
        "    model.w = model.w - self.lr * tf.reduce_sum(grads[0],axis=0,keepdims=True)\n",
        "    model.b = model.b - self.lr  * tf.reduce_sum(grads[1],axis=0,keepdims=True)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqAZAPWNsVRV",
        "outputId": "0d19e1a1-8ef6-49ca-d074-78ee5938555f"
      },
      "source": [
        "# Evaluation\n",
        "model_0 = Linear()\n",
        "opt_0 = GD()\n",
        "print(loss(model_0(X_train), y_train).shape)\n",
        "\n",
        "print(\"Initial loss: %.3f\" % loss(model_0(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_0.calculate_gradient(X_train, y_train, model_0)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_0.apply_gradient(grads, model_0)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_0(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_0(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_0.w.numpy(), model_0.b.numpy()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 1)\n",
            "Initial loss: 68.725\n",
            "Loss at epoch 0: 66.072\n",
            "Loss at epoch 20: 30.250\n",
            "Loss at epoch 40: 14.141\n",
            "Loss at epoch 60: 6.895\n",
            "Loss at epoch 80: 3.636\n",
            "Loss at epoch 100: 2.169\n",
            "Loss at epoch 120: 1.509\n",
            "Loss at epoch 140: 1.212\n",
            "Loss at epoch 160: 1.078\n",
            "Loss at epoch 180: 1.018\n",
            "Loss at epoch 200: 0.991\n",
            "Loss at epoch 220: 0.979\n",
            "Loss at epoch 240: 0.973\n",
            "Loss at epoch 260: 0.971\n",
            "Loss at epoch 280: 0.970\n",
            "Final loss: 0.969\n",
            "w = [[4.0046396 3.0552888]], b = [[2.0367746]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qB8UveuntaAd",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f29788020d6f3c633900f1c3b5174d46",
          "grade": true,
          "grade_id": "test_GD",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_0(X_train), y_train), 0.969, atol=1e-3), 'Wrong implementation of GD'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gpEJl78dWkW"
      },
      "source": [
        "## (2b) Stochastic Gradient Descent\n",
        "Let's take one step further. In SGD, you should randomly choose one data point to calculate the gradient each time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "U-uvm_TCdqTN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "111f3947040e52d9b536071859a9ee39",
          "grade": false,
          "grade_id": "SGD",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "class SGD():\n",
        "  def __init__(self, lr=0.01):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "    \"\"\"\n",
        "    self.lr = lr \n",
        "\n",
        "  def calculate_gradient(self, X, y, model):\n",
        "    \"\"\"Calculates gradients in the computational graph.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          X (tf.Tensor): The input.\n",
        "          y (tf.Tensor): The actual label.\n",
        "          model (tf.keras.Model): The model used to predict y with given input X. \n",
        "\n",
        "      Returns:\n",
        "          Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "    \"\"\"\n",
        "    import random \n",
        "\n",
        "    # number of samples\n",
        "    N = len(y)\n",
        "    # randomly select a training example uniformly\n",
        "    i = random.randint(0, N-1)\n",
        "    xi = tf.expand_dims(X[i,:], axis=0)\n",
        "    yi = y[i,:]\n",
        "\n",
        "    # difference between predicted, actual\n",
        "    diff = model(xi) - yi\n",
        "    # gradients\n",
        "    grads_w =  2*diff*xi\n",
        "    grads_b =  2*diff\n",
        "    return (grads_w, grads_b)\n",
        "\n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        "    \"\"\"Updates parameters of the model.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          grads (Tuple): (gradient w.r.t W, gradient w.r.t B).\n",
        "          model (tf.keras.Model): The model used to predict y with given input X. \n",
        "    \"\"\"\n",
        "\n",
        "    # update weight, bias\n",
        "    model.w = model.w - self.lr * grads[0]\n",
        "    model.b = model.b - self.lr * grads[1]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F5K9CzguDBq",
        "outputId": "1408fa6d-444f-4f92-bfd7-1ca36818229b"
      },
      "source": [
        "# Evaluation\n",
        "model_1 = Linear()\n",
        "opt_1 = SGD()\n",
        "\n",
        "print(\"Initial loss: %.3f\" % loss(model_1(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_1.calculate_gradient(X_train, y_train, model_1)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_1.apply_gradient(grads, model_1)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_1(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_1(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_1.w.numpy(), model_1.b.numpy()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial loss: 68.725\n",
            "Loss at epoch 0: 65.354\n",
            "Loss at epoch 20: 26.013\n",
            "Loss at epoch 40: 12.925\n",
            "Loss at epoch 60: 5.691\n",
            "Loss at epoch 80: 3.200\n",
            "Loss at epoch 100: 1.991\n",
            "Loss at epoch 120: 1.456\n",
            "Loss at epoch 140: 1.221\n",
            "Loss at epoch 160: 1.174\n",
            "Loss at epoch 180: 1.126\n",
            "Loss at epoch 200: 1.057\n",
            "Loss at epoch 220: 1.006\n",
            "Loss at epoch 240: 0.994\n",
            "Loss at epoch 260: 1.010\n",
            "Loss at epoch 280: 0.996\n",
            "Final loss: 1.001\n",
            "w = [[4.0619526 2.8951478]], b = [[1.9478302]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fYQkHSwGuOC_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3cfb94a77ae887117d88c6816d5cdc7d",
          "grade": true,
          "grade_id": "test_SGD",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_1(X_train), y_train), 1.0, atol=0.05), 'Wrong implementation of SGD'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIkQubmCuWWy"
      },
      "source": [
        "## (2c) AdaGrad\n",
        "\n",
        "One of the shortcomings about Gradient Descent is that its performance highly depends on the initial learning rate. Poor selection of initial learning rate would lead to either slow convergence or incapability of finding local minimum. In order to solve this problem, a revised version called AdaGrad is introduced. \n",
        "\n",
        "As its name suggests, AdaGrad makes its learning rate adaptive. To be more specific, AdaGrad uses past squared gradients to form an accumulated regularizer for its learning rate. \n",
        "\n",
        "Assuming $g^{(t)}$ is the gradient at $t$ th epoch, AdaGrad first calculates the accumulated squared gradients:\n",
        "\n",
        "$$r^{(t)}=r^{(t-1)}+(g^{(t)})^2$$\n",
        "\n",
        "Then AdaGrad will use this accumulated squared gradients to regularize its learning rate:\n",
        "\n",
        "$$w^{(t+1)}=w^{(t)}-\\frac{\\alpha}{\\sqrt{r^{(t)}} + \\epsilon} \\cdot g_w^{(t)}$$\n",
        "$$b^{(t+1)}=b^{(t)}-\\frac{\\alpha}{\\sqrt{r^{(t)}} + \\epsilon} \\cdot g_b^{(t)}$$\n",
        "\n",
        "where $\\epsilon$ is an additive constant (usually set as $10^{-7}$) that ensures we do not divide by 0. \n",
        "\n",
        "Note that you can reuse the `calculate_gradient` function in SGD for AdaGrad. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "dXcODnEjzrz1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e8b721afab643bf082a7ce14299296d8",
          "grade": false,
          "grade_id": "AdaGrad",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "class AdaGrad():\n",
        "  def __init__(self, lr=0.001, epsilon=1e-7):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Note: self.accumulator is the accumulated squared gradients. It is a tuple with following format:\n",
        "        (accumulator w.r.t w, accumulator w.r.t b)\n",
        "        And it should be initialized with the value of 0.1\n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "          epsilon (float): Additive constant that ensures we do not divide by 0.\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "    self.epsilon = epsilon\n",
        "    self.accumulator = [[[0.1,0.1]],[[0.1]]]\n",
        "\n",
        "    print(\"initial accum0\", self.accumulator[0])\n",
        "    \n",
        "  def calculate_gradient(self, X, y, model):\n",
        "    \"\"\"Calculates gradients in the computational graph.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          X (tf.Tensor): The input.\n",
        "          y (tf.Tensor): The actual label.\n",
        "          model (tf.keras.Model): The model used to predict y with given input X. \n",
        "\n",
        "      Returns:\n",
        "          Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "    \"\"\"\n",
        "    # number of samples\n",
        "    N = len(y)\n",
        "    # randomly select a training example uniformly\n",
        "    i = random.randint(0, N-1)\n",
        "    xi = tf.expand_dims(X[i,:], axis=0)\n",
        "    yi = y[i,:]\n",
        "\n",
        "    # difference between predicted, actual\n",
        "    diff = model(xi) - yi\n",
        "    # gradients\n",
        "    grads_w =  2*diff*xi\n",
        "    grads_b =  2*diff\n",
        "    return (grads_w, grads_b)\n",
        "\n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        "    \"\"\"Updates parameters of the model.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          grads (Tuple): (gradient w.r.t w, gradient w.r.t b).\n",
        "          model (tf.keras.Model): The model used to predict y with given input X. \n",
        "    \"\"\"\n",
        "    \n",
        "    # accumulator update\n",
        "    self.accumulator[0] += tf.square(grads[0])\n",
        "    self.accumulator[1] += tf.square(grads[1])\n",
        "\n",
        "    # update weight, bias\n",
        "    model.w = model.w - (self.lr * grads[0]) / tf.add(tf.sqrt(self.accumulator[0]),self.epsilon)\n",
        "    model.b = model.b - (self.lr * grads[1]) / tf.add(tf.sqrt(self.accumulator[1]),self.epsilon)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzeYXHRZ2xZD",
        "outputId": "450f0f9d-60f7-49b3-832e-9ef2ff4fc646"
      },
      "source": [
        "# Evaluation\n",
        "model_2 = Linear()\n",
        "opt_2 = AdaGrad(lr=0.1)\n",
        "\n",
        "print(\"Initial loss: %.3f\" % loss(model_2(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_2.calculate_gradient(X_train, y_train, model_2)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_2.apply_gradient(grads, model_2)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_2(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_2(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_2.w.numpy(), model_2.b.numpy()))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial accum0 [[0.1, 0.1]]\n",
            "Initial loss: 68.725\n",
            "Loss at epoch 0: 67.743\n",
            "Loss at epoch 20: 57.301\n",
            "Loss at epoch 40: 52.710\n",
            "Loss at epoch 60: 49.344\n",
            "Loss at epoch 80: 46.145\n",
            "Loss at epoch 100: 43.669\n",
            "Loss at epoch 120: 41.671\n",
            "Loss at epoch 140: 39.348\n",
            "Loss at epoch 160: 37.575\n",
            "Loss at epoch 180: 36.256\n",
            "Loss at epoch 200: 34.594\n",
            "Loss at epoch 220: 33.504\n",
            "Loss at epoch 240: 32.218\n",
            "Loss at epoch 260: 30.709\n",
            "Loss at epoch 280: 29.897\n",
            "Final loss: 28.979\n",
            "w = [[4.7232394 4.5006275]], b = [[7.0889926]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "NhDsd7ge3DSh",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "51d3f5fa96af35d1ac41b8f88304daae",
          "grade": true,
          "grade_id": "test_AdaGrad",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_2(X_train), y_train), 29.0, atol=2.0), 'Wrong implementation of AdaGrad'\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfBS1R4Y3Kzh"
      },
      "source": [
        "## (2d) Adam\n",
        "Although it sounds like a powerful algorithm, AdaGrad still has a bunch of shortcomings. One of them is that as the accumulated squared gradients becoming larger, the learning rate will be pushed to 0, leading to an early stop. \n",
        "\n",
        "Adam is designed to improve AdaGrad. It relies on adaptive moment estimation to set an independent adaptive learning rate for each parameter. The idea of Adam at the $t$ th epoch is described below:\n",
        "- Calculate current gradient $g^{(t)}$\n",
        "- Update biased first moment estimate\n",
        "  $$m^{(t)} = \\beta_1\\cdot m^{(t-1)}+(1-\\beta_1)\\cdot g^{(t)}$$\n",
        "- Update biased second raw moment estimate\n",
        "  $$v^{(t)} = \\beta_2\\cdot v^{(t-1)}+(1-\\beta_2)\\cdot (g^{(t)})^2$$\n",
        "- Compute bias-corrected first moment estimate\n",
        "  $$\\hat{m}^{(t)} = \\frac{m^{(t)}}{1-\\beta_1^t}$$\n",
        "- Compute bias-corrected second raw moment estimate\n",
        "  $$\\hat{v}^{(t)} = \\frac{v^{(t)}}{1-\\beta_2^t}$$\n",
        "- Update parameters with constrained learning rate\n",
        "  $$w^{(t)} = w^{(t-1)}-\\alpha\\cdot\\frac{\\hat{m}_w^{(t)}}{(\\sqrt{\\hat{v}_w^{(t)}}+\\epsilon)}$$\n",
        "  $$b^{(t)} = b^{(t-1)}-\\alpha\\cdot\\frac{\\hat{m}_b^{(t)}}{(\\sqrt{\\hat{v}_b^{(t)}}+\\epsilon)}$$\n",
        "\n",
        "where $\\beta_1$ and $\\beta_2$ are hyperparameters for the first and the second moment estimation\n",
        "\n",
        "As we can see from the update rule, Adam dynamically forms boundaries for its learning rate using first and second moment estimate without requiring too much memory space. \n",
        "\n",
        "Note that you can reuse the `calculate_gradient` function in SGD for Adam. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "id": "FfEvvKMPBGIF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "203d4e2eb49f88a71afb37aa0b0f6208",
          "grade": false,
          "grade_id": "Adam",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "class Adam():\n",
        "  def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
        "    \"\"\"Initializes required parameters.\n",
        "\n",
        "      Note: \n",
        "        self.m is a list of Tensor, representing first moment estimations of gradients. The initial value should be [0]. \n",
        "        self.v is a list of Tensor, representing second moment estimations of gradients. The initial value should be [0]. \n",
        "        self.t is the timestep. The initial value should be 0. \n",
        "\n",
        "      Args:\n",
        "          lr (float): Learning rate.\n",
        "          epsilon (float): Additive constant that ensures we do not divide by 0.\n",
        "          beta1 (float): Hyperparameter for first moment estimation\n",
        "          beta2 (float): Hyperparameter for second moment estimation\n",
        "    \"\"\"\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.mw = 0.0\n",
        "    self.mb = 0.0\n",
        "    self.vw = 0.0\n",
        "    self.vb = 0.0\n",
        "\n",
        "    self.t = 0\n",
        "    \n",
        "    \n",
        "  def calculate_gradient(self, X, y, model):\n",
        "    \"\"\"Calculates gradients in the computational graph.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          X (tf.Tensor): The input.\n",
        "          y (tf.Tensor): The actual label.\n",
        "          model (tf.keras.Model): The model used to predict y with given input X. \n",
        "\n",
        "      Returns:\n",
        "          Tuple: (gradient w.r.t w, gradient w.r.t b)\n",
        "    \"\"\"\n",
        "\n",
        "    # number of samples\n",
        "    N = len(y)\n",
        "    # randomly select a training example uniformly\n",
        "    i = random.randint(0, N-1)\n",
        "    xi = tf.expand_dims(X[i,:], axis=0)\n",
        "    yi = y[i,:]\n",
        "\n",
        "    # difference between predicted, actual\n",
        "    diff = model(xi) - yi\n",
        "    # gradients\n",
        "    grads_w =  2*diff*xi\n",
        "    grads_b =  2*diff\n",
        "    return (grads_w, grads_b)\n",
        "    \n",
        "\n",
        "  def apply_gradient(self, grads, model):\n",
        "    \"\"\"Updates parameters of the model.\n",
        "\n",
        "      Note:\n",
        "          You should try to make use of Tensorflow library to perform related calculation.\n",
        "\n",
        "      Args:\n",
        "          grads (Tuple): (gradient w.r.t w, gradient w.r.t b).\n",
        "          model (tf.keras.Model): The model used to predict y with given input X. \n",
        "    \"\"\"\n",
        "\n",
        "    # increment timestep \n",
        "    self.t+=1\n",
        "   \n",
        "    # biased first moment \n",
        "    mw =  self.beta1*self.mw + (1-self.beta1)*grads[0]\n",
        "    mb =  self.beta1*self.mb + (1-self.beta1)*grads[1]\n",
        "\n",
        "    # biased second moment \n",
        "    vw =  self.beta2*self.vw + (1-self.beta2)*tf.square(grads[0])\n",
        "    vb =  self.beta2*self.vb + (1-self.beta2)*tf.square(grads[1])\n",
        "\n",
        "    # store new moments for each parameter \n",
        "    self.mw = mw\n",
        "    self.mb = mb\n",
        "    self.vw = vw\n",
        "    self.vb = vb\n",
        "\n",
        "  \n",
        "    # normalize\n",
        "    mw_norm = mw / (1-tf.pow(self.beta1,self.t))\n",
        "    vw_norm = vw / (1-tf.pow(self.beta2,self.t))\n",
        "    mb_norm = mb / (1-tf.pow(self.beta1,self.t))\n",
        "    vb_norm = vb / (1-tf.pow(self.beta2,self.t))\n",
        "\n",
        "    # update weights \n",
        "    model.w = model.w - ((self.lr * mw_norm) / (tf.sqrt(vw_norm) + self.epsilon))\n",
        "    model.b = model.b - ((self.lr * mb_norm) / (tf.sqrt(vb_norm)+ self.epsilon))\n",
        "    \n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0ImykFnDgGf",
        "outputId": "df923782-deaa-4faf-9ee8-a38e6de8d415"
      },
      "source": [
        "# Evaluation\n",
        "model_3 = Linear()\n",
        "opt_3 = Adam(lr=0.1)\n",
        "print(\"Initial loss: %.3f\" % loss(model_3(X_train), y_train))\n",
        "\n",
        "epochs = 300\n",
        "for i in range(epochs):\n",
        "  # Calculate current gradients\n",
        "  grads = opt_3.calculate_gradient(X_train, y_train, model_3)\n",
        "  # Use the optimizer to update gradients\n",
        "  opt_3.apply_gradient(grads, model_3)\n",
        "  if i % 20 == 0:\n",
        "    print(\"Loss at epoch %d: %.3f\" % (i, loss(model_3(X_train), y_train)))\n",
        "\n",
        "print(\"Final loss: %.3f\" % loss(model_3(X_train), y_train))\n",
        "print(\"w = %s, b = %s\" % (model_3.w.numpy(), model_3.b.numpy()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial loss: 68.725\n",
            "Loss at epoch 0: 67.015\n",
            "Loss at epoch 20: 40.483\n",
            "Loss at epoch 40: 22.323\n",
            "Loss at epoch 60: 11.619\n",
            "Loss at epoch 80: 4.828\n",
            "Loss at epoch 100: 2.023\n",
            "Loss at epoch 120: 1.207\n",
            "Loss at epoch 140: 1.049\n",
            "Loss at epoch 160: 1.025\n",
            "Loss at epoch 180: 0.988\n",
            "Loss at epoch 200: 0.991\n",
            "Loss at epoch 220: 0.976\n",
            "Loss at epoch 240: 0.983\n",
            "Loss at epoch 260: 0.999\n",
            "Loss at epoch 280: 0.991\n",
            "Final loss: 1.009\n",
            "w = [[3.9566288 2.9348269]], b = [[2.166669]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "T4WougDyDswB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9b20bbff8cafa63968a8d947858b2f3",
          "grade": true,
          "grade_id": "test_Adam",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Test case\n",
        "assert np.allclose(loss(model_3(X_train), y_train), 1.0, atol=0.4), 'Wrong implementation of Adam'\n"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}